# GPT-scratch
Simple implementation of a GPT from scratch using a stack of decoder blocks.  

## References
[\[1\] "Attention Is All You Need" paper](https://arxiv.org/abs/1706.03762)  
[\[2\] "Transformers From Scratch" by Peter Bloem](https://peterbloem.nl/blog/transformers)
